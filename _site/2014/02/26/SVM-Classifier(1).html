<!DOCTYPE html>
<html>
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
        <title>[Machine Learning] 支持向量机介绍 (1)</title>
        <meta name="viewport" content="width=device-width">

        <!-- syntax highlighting CSS -->
        <link rel="stylesheet" href="/css/syntax.css">

        <!-- Custom CSS -->
        <link rel="stylesheet" href="/css/main.css">

        <!-- Import MathJax -->
        <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

    </head>
    <body>

        <div class="site">
          <div class="header">
            <h1 class="title"><a href="/">Waldronluo's Blog</a></h1>
            <a class="extra" href="/">home</a>
          </div>

          <h2>[Machine Learning] 支持向量机介绍 (1)</h2>
<p class="meta">26 Feb 2014</p>

<div class="post">
<p>首先声明，这篇东西是我学习SVM时候看的材料。其作者为Standford大学的Andrew Ng，而这篇东西是网易公开课Andrew Ng的机器学习课程的课程材料。而最近我学习使用SVM来进行力信号分析。所以斗胆翻译一下我现在使用的课程材料权当学习。若是侵犯的版权，我会删掉的啦。（P.S. 然后大家最好开个翻墙，规则加入MathJax.org，否则有可能看不见可怜的公式们呢。）</p>

<h1 id="support-vector-machine">Support Vector Machine</h1>

<p>以下这个系列呐我们来讲讲Support Vector Machine(SVM) 学习算法。SVM是目前最好（至少是好多人这么觉得的）最“畅销”的监督学习算法。首先呢我们得先讲讲间隔（margin），用以区分数据的”鸿沟”.其次我们会说下最优间隔分类（optimal margin classifier），此时我们会接触到拉格朗日对偶法（Lagrange duality）。然后我们会说下核函数（Kernel），它使我们在高维甚至无限维空间中依然可以有效地使用SVM。最后，我们会使用SMO，来演示如何实现一个高效的SVM。</p>

<h3 id="margin">直观点，来说说间隔(Margin)</h3>

<p>我们的故事始于对于间隔（Margin）的直观认识。这小节会从间隔开始，告诉你怎么看预测的可信程度。在第三小节里面我们会正式论述它们。<br />
来，先回忆下逻辑回归（Logistic Regression），它的概率函数<script type="math/tex">{p(y=1|x;\theta)}</script>是由<script type="math/tex">{h_\theta(x) = g(\theta^{T}x)}</script>建模得到的。在输入<script type="math/tex">{x}</script>之后，我们可以根据<script type="math/tex">{h_{\theta}(x)\geq0.5}</script>来判断其预测值是否为1。当其预测值为1时，<script type="math/tex">\theta^{T}x\geq0</script>。这是一个充要条件。现在再回忆一下，当<script type="math/tex">(y=1)</script>时，<script type="math/tex">\theta^{T}x</script>越大，也就是说<script type="math/tex">h_{\theta}(x)=p(y=1|x;w,b)</script>越大，预测值为1的可信度就越高。因此，当<script type="math/tex">\theta^{T}x\gg0</script>时，我们对它被预测为1这个结果信心满满；反之，当<script type="math/tex">\theta^{T}x\ll0</script>时，我们相信它就该被预测为0，虽然这么说有点不专业，但是也没啥问题。给定一个训练集，若是里面每个<script type="math/tex">y^{(i)}</script>1的案例其<script type="math/tex">\theta^{T}x^{(i)}\gg0</script>，每个<script type="math/tex">y^{(i)}</script>的案例<script type="math/tex">\theta^{T}x^{(i)}\ll0</script>，我们认为这就是一个好的拟合，因为每个案例的预测都表现出了很高的可信度。这样的结果是我们超想要的。接下来我们就以函数间隔（functional margins）来专业地描述一下这个问题。<br />
我们来考虑一下下面这张图，不同于上面的直观认识，我们需要画出一条直线将标记为x和标记为o的数据分开（这条线的方程为<script type="math/tex">\theta^{T}x=0</script>，叫做分隔平面）。图中，还有标记为A、B、C这么三个点。<br />
<img src="/picture/svm/svmClassifier1.png" width="628" />
现在点A距离这条直线是有够远的。假如此时我们来判断A方位的y的值，看起来我们就该将其判断为1。相反，C就离这条直线挺近的，但因为它在线的上方，我们会把它预测为1,可是好像只要可能的一点点的变化，我们就得把它预测成0了。而B在这两点的中间。此时，我们可以看出，当一个点越远离分隔平面时，我们就对其预测值越有信心。也就是说，跟逻辑回归一样，假如对于一个训练集，我们找到的作为分隔平面的直线，它距离这个训练集的点越远，这条直线就越好，这个预测就越有可信度。我们将直观地使用几何距离这个说法来表示它。</p>

</div>


          <div class="footer">
            <div class="contact">
              <p>
                Waldron Luo<br />
                Student in SYSU<br />
                luoweiq2@mail2.sysu.edu.cn
              </p>
            </div>
            <div class="contact">
              <p>
                <a href="https://github.com/waldronluo">github.com/waldronluo</a><br />
                <a href="http://cn.linkedin.com/pub/weiqiang-luo/80/1ba/6b6/">cn.linkedin.com/pub/weiqiang-luo</a><br />
              </p>
            </div>
          </div>
        </div>

    </body>
</html>
