---
layout: post
title: "[Machine Learning] 支持向量机介绍 (1)"
data: 2014-02-26 10:48:25
catagories: SVM MachineLearning Translation 
---

首先声明，这篇东西是我学习SVM时候看的材料。其作者为Standford大学的Andrew Ng，而这篇东西是网易公开课Andrew Ng的机器学习课程的课程材料。而最近我学习使用SVM来进行力信号分析。所以斗胆翻译一下我现在使用的课程材料权当学习。若是侵犯的版权，我会删掉的啦。（P.S. 然后大家最好开个翻墙，规则加入MathJax.org，否则有可能看不见可怜的公式们呢。）

Support Vector Machine
======================

以下这个系列呐我们来讲讲Support Vector Machine(SVM) 学习算法。SVM是目前最好（至少是好多人这么觉得的）最“畅销”的监督学习算法。首先呢我们得先讲讲间隔（margin），用以区分数据的"鸿沟".其次我们会说下最优间隔分类（optimal margin classifier），此时我们会接触到拉格朗日对偶法（Lagrange duality）。然后我们会说下核函数（Kernel），它使我们在高维甚至无限维空间中依然可以有效地使用SVM。最后，我们会使用SMO，来演示如何实现一个高效的SVM。

### 直观点，来说说间隔(Margin)

我们的故事始于对于间隔（Margin）的直观认识。这小节会从间隔开始，告诉你怎么看预测的可信程度。在第三小节里面我们会正式论述它们。  
来，先回忆下逻辑回归（Logistic Regression），它的概率函数$${p(y=1|x;\theta)}$$是由$${h_\theta(x) = g(\theta^{T}x)}$$建模得到的。在输入$${x}$$之后，我们可以根据$${h_{\theta}(x)\geq0.5}$$来判断其预测值是否为1。当其预测值为1时，$$\theta^{T}x\geq0$$。这是一个充要条件。现在再回忆一下，当$$(y=1)$$时，$$\theta^{T}x$$越大，也就是说$$h_{\theta}(x)=p(y=1|x;w,b)$$越大，预测值为1的可信度就越高。因此，当$$\theta^{T}x\gg0$$时，我们对它被预测为1这个结果信心满满；反之，当$$\theta^{T}x\ll0$$时，我们相信它就该被预测为0，虽然这么说有点不专业，但是也没啥问题。给定一个训练集，若是里面每个$$y^{(i)}$$1的案例其$$\theta^{T}x^{(i)}\gg0$$，每个$$y^{(i)}$$的案例$$\theta^{T}x^{(i)}\ll0$$，我们认为这就是一个好的拟合，因为每个案例的预测都表现出了很高的可信度。这样的结果是我们超想要的。接下来我们就以函数间隔（functional margins）来专业地描述一下这个问题。  
我们来考虑一下下面这张图，不同于上面的直观认识，我们需要画出一条直线将标记为x和标记为o的数据分开（这条线的方程为$$\theta^{T}x=0$$，叫做分隔平面）。图中，还有标记为A、B、C这么三个点。  
<img src="/picture/svm/svmClassifier1.png" width="628"/>
现在点A距离这条直线是有够远的。假如此时我们来判断A方位的y的值，看起来我们就该将其判断为1。相反，C就离这条直线挺近的，但因为它在线的上方，我们会把它预测为1,可是好像只要可能的一点点的变化，我们就得把它预测成0了。而B在这两点的中间。此时，我们可以看出，当一个点越远离分隔平面时，我们就对其预测值越有信心。也就是说，跟逻辑回归一样，假如对于一个训练集，我们找到的作为分隔平面的直线，它距离这个训练集的点越远，这条直线就越好，这个预测就越有可信度。我们将直观地使用几何距离这个说法来表示它。
